# -*- coding: utf-8 -*-
"""Week6Lab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PY2Bsn2gszLGcA8eMyOnguhghU5STxep

Matt Falcona Week 6 Lab
"""

#!pip install fbprophet

# import packages for analysis and modeling
import pandas as pd  # data frame operations
# from pandas.tools.plotting import scatter_matrix  # scatter plot matrix
from pandas import Series
import numpy as np  # arrays and math functions
from scipy.stats import uniform  # for training-and-test split
import statsmodels.api as sm  # statistical models (including regression)
import statsmodels.formula.api as smf  # R-like model specification
from sklearn.tree import DecisionTreeRegressor  # machine learning tree
from sklearn.ensemble import RandomForestRegressor # ensemble method
from matplotlib import pyplot
import seaborn as sns

import timeit
from fbprophet import Prophet

import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# reading in data set

zillow = pd.read_csv('https://files.zillowstatic.com/research/public/Zip/Zip_Zhvi_SingleFamilyResidence.csv')

# examining zillow data set

zillow.info()
zillow.shape
zillow.head()

# dataset is way too wide, need to make it narrow
# using pandas melt function

zillow_narrow = pd.melt(zillow, id_vars=['RegionID','SizeRank','RegionName','RegionType','StateName','State','City','Metro','CountyName'], var_name = 'Date', value_name = 'MeanHomeValue')
zillow_narrow.head()

# checking for null values

zillow_narrow.isnull().any()

# removing rows with NAs for Metro and MeanHomeValue - old data does not have impact on current housing prices

zillow_narrow = zillow_narrow.dropna()

# checking for null values

zillow_narrow.head()

# checking data types

zillow_narrow.info()

# let's change everything to string, except Date (datetime) and MeanHomeValue (keep as float)

# adjusting data types

zillow_narrow['RegionID'] = zillow_narrow['RegionID'].astype(str)
zillow_narrow['SizeRank'] = zillow_narrow['SizeRank'].astype(str)
zillow_narrow['RegionName'] = zillow_narrow['RegionName'].astype(str)
zillow_narrow['RegionType'] = zillow_narrow['RegionType'].astype(str)
zillow_narrow['StateName'] = zillow_narrow['StateName'].astype(str)
zillow_narrow['State'] = zillow_narrow['State'].astype(str)
zillow_narrow['City'] = zillow_narrow['City'].astype(str)
zillow_narrow['Metro'] = zillow_narrow['Metro'].astype(str)
zillow_narrow['CountyName'] = zillow_narrow['Metro'].astype(str)
zillow_narrow['Date'] = pd.to_datetime(zillow_narrow['Date'])

zillow_narrow.info()

# changing RegionName to ZipCode for clarity

zillow_narrow = zillow_narrow.rename(index=str, columns={"RegionName": "ZipCode"})

zillow_narrow.head()

# initial data analysis: Arkansas metro areas

# ark_metro = ['Hot Springs','Fayetteville-Springdale-Rogers','Little Rock-North Little Rock-Conway','Searcy']

hogs = zillow_narrow[zillow_narrow['Metro'].isin(['Hot Springs','Fayetteville-Springdale-Rogers','Little Rock-North Little Rock-Conway','Searcy'])]

hogs.head()

# woo pig sooie

# examining basic statistics for Arkansas metro dataset

hogs.MeanHomeValue.describe()

# let's remove rows with missing data for the purposes of our analysis

hogs = hogs[hogs['MeanHomeValue'] != 0]

hogs.MeanHomeValue.describe()

# histogram of mean home values for each metro area

plt.hist(hogs[hogs['Metro']=='Hot Springs']['MeanHomeValue'])
plt.xticks(rotation = 45, fontsize = 8)
plt.yticks(fontsize = 8)
plt.xlabel('Mean Home Value ($)', fontsize = 12)
plt.ylabel('Count', fontsize = 12)
plt.title('Histogram of Hot Springs, Ark Metro Home Values', fontsize = 16)
plt.show()

# histogram of mean home values for each metro area

plt.hist(hogs[hogs['Metro']=='Fayetteville-Springdale-Rogers']['MeanHomeValue'])
plt.xticks(rotation = 45, fontsize = 8)
plt.yticks(fontsize = 8)
plt.xlabel('Mean Home Value ($)', fontsize = 12)
plt.ylabel('Count', fontsize = 12)
plt.title('Histogram of Fayetteville, Ark Metro Home Values', fontsize = 16)
plt.show()

# histogram of mean home values for each metro area

plt.hist(hogs[hogs['Metro']=='Little Rock-North Little Rock-Conway']['MeanHomeValue'])
plt.xticks(rotation = 45, fontsize = 8)
plt.yticks(fontsize = 8)
plt.xlabel('Mean Home Value ($)', fontsize = 12)
plt.ylabel('Count', fontsize = 12)
plt.title('Histogram of Little Rock, Ark Metro Home Values', fontsize = 16)
plt.show()

# histogram of mean home values for each metro area

plt.hist(hogs[hogs['Metro']=='Searcy']['MeanHomeValue'])
plt.xticks(rotation = 45, fontsize = 8)
plt.yticks(fontsize = 8)
plt.xlabel('Mean Home Value ($)', fontsize = 12)
plt.ylabel('Count', fontsize = 12)
plt.title('Histogram of Searcy, Ark Metro Home Values', fontsize = 16)
plt.show()

# time series plot for each metro area

# first, let's create a new timeseries df, removing all data except date (index), metro and Mean Home Value
ark_ts = hogs

ark_ts = ark_ts.drop(columns = ['RegionID','SizeRank','ZipCode','RegionType','StateName','State','City','CountyName'])
ark_ts = ark_ts.set_index('Date')

ark_ts.head()

# Hot Springs time series plot

sns.lineplot(x = 'Date', y = 'MeanHomeValue', data = ark_ts[ark_ts['Metro'] == 'Hot Springs'])
plt.title('Hot Springs Metro Home Values, 1996-2020')
plt.show()

# Fayetteville time series plot

sns.lineplot(x = 'Date', y = 'MeanHomeValue', data = ark_ts[ark_ts['Metro'] == 'Fayetteville-Springdale-Rogers'])
plt.title('Fayetteville Metro Home Values, 1996-2020')
plt.show()

# Little Rock time series plot

sns.lineplot(x = 'Date', y = 'MeanHomeValue', data = ark_ts[ark_ts['Metro'] == 'Little Rock-North Little Rock-Conway'])
plt.title('Little Rock Metro Home Values, 1996-2020')
plt.show()

# Searcy time series plot

sns.lineplot(x = 'Date', y = 'MeanHomeValue', data = ark_ts[ark_ts['Metro'] == 'Searcy'])
plt.title('Searcy Metro Home Values, 1996-2020')
plt.show()

# creating a new timeseries df for entire dataset (same as Ark, except replacing Zip with metro)

zillow_ts = zillow_narrow

zillow_ts = zillow_ts.drop(columns = ['RegionID','SizeRank','Metro','RegionType','StateName','State','City','CountyName'])

zillow_ts = zillow_ts.set_index('ZipCode')

zillow_ts.head()

# splitting data into training and testing sets
# remvoving data prior to 2010 (dataset is too big for prophet to handle, and 10+ years old housing data is not very relevant)

cutoff_date = '2018-01-01'
cutoff_2 = '2010-01-01'

train = zillow_ts[(zillow_ts['Date'] < cutoff_date) & (zillow_ts['Date'] > cutoff_2)]
train = train.rename(columns={'Date' : 'ds', 'MeanHomeValue' : 'y'})

test = zillow_ts[zillow_ts['Date'] > cutoff_date]
test = test.rename(columns={'Date' : 'ds', 'MeanHomeValue' : 'y'})

train.head()

# setting uncertainty interval to 95%
z_model = Prophet(interval_width=.95)
z_model.fit(train)

# making new df with future values

future_home_values = z_model.make_future_dataframe(periods = 40, freq = 'M')
future_home_values.tail()

# base model prediction

forecast = z_model.predict(future_home_values)

# examining data types for forecast df

forecast.info()

# observing forecasted values for 2021

forecast.tail()

"""prophet drops the index (zip code) so it is impossible to make projects for the whole country. Instead, I will make projections for each of the top 5 zip codes in the country based on percent change in home value over the relevant time period (2010-present)"""

# taking zip code, first day of 2010, and last day of 2020

top_zip = zillow[['RegionName','2010-01-31','2020-03-31']]
top_zip.head()

# calculating change in home value from beginning to end of dataset

top_zip['change'] = top_zip['2020-03-31'] - top_zip['2010-01-31']
top_zip['pct_change'] = top_zip['change']/top_zip['2010-01-31']
top_zip.head()

# soring df by percent change (descending) to determine zip codes with highest change

top_zip = top_zip.sort_values(by = ['pct_change'], ascending=False)
top_zip.head()

# top 5 zips by % change are 91108, 90211, 90403, 94610, 90266.

# creating df for top zip change bar plot

top_zip_plot = top_zip[top_zip['pct_change'] > 2.5]
top_zip_plot = top_zip_plot.sort_values(by = ['pct_change'], ascending=False)
top_zip_plot.head()

# creating bar plot of top zip codes by percent change in home value

sns.set(font_scale=1)
sns.barplot(x = 'RegionName', y = 'pct_change', data = top_zip_plot, palette= 'Blues_d')
plt.xlabel('Zip Code')
plt.ylabel('Percent Change')
plt.title('U.S Zip Codes with >2.5% Increase in Home Value from 2010-2020')
plt.xticks(rotation = 45)

# now, lets run a forecast for each of these zip codes using modified train and test sets.
# Using prophet's forecast, we can see how much these property values will rise in the next 10 years and which 3 will be best to invest in.
# pulling the top 5 zip codes by percent change 2010-2020

train_91108 = train[train.index.isin(['91108'])]
train_90211 = train[train.index.isin(['90211'])]
train_90403 = train[train.index.isin(['90403'])]
train_94610 = train[train.index.isin(['94610'])]
train_90266 = train[train.index.isin(['90266'])]
test_91108 = test[test.index.isin(['91108'])]
test_90211 = test[test.index.isin(['90211'])]
test_90403 = test[test.index.isin(['90403'])]
test_94610 = test[test.index.isin(['94610'])]
test_90266 = test[test.index.isin(['90266'])]

train_90211.head()

test_90266.head()

# setting uncertainty interval to 95%
model_91108 = Prophet(interval_width=.95)
model_91108.fit(train_91108)
model_90211 = Prophet(interval_width=.95)
model_90211.fit(train_90211)
model_90403 = Prophet(interval_width=.95)
model_90403.fit(train_90403)
model_94610 = Prophet(interval_width=.95)
model_94610.fit(train_94610)
model_90266 = Prophet(interval_width=.95)
model_90266.fit(train_90266)

# 144 periods = 12 years in future from end of training set. Forecasting until 2030
# creating model for each zip code

fv_91108 = model_91108.make_future_dataframe(periods = 144, freq = 'M')
fv_90211 = model_90211.make_future_dataframe(periods = 144, freq = 'M')
fv_90403 = model_90403.make_future_dataframe(periods = 144, freq = 'M')
fv_94610 = model_94610.make_future_dataframe(periods = 144, freq = 'M')
fv_90266 = model_90266.make_future_dataframe(periods = 144, freq = 'M')

# creating forecast for each zip code

forecast_91108 = model_91108.predict(fv_91108)
forecast_90211 = model_90211.predict(fv_90211)
forecast_90403 = model_90403.predict(fv_90403)
forecast_94610 = model_94610.predict(fv_94610)
forecast_90266 = model_90266.predict(fv_90266)

forecast_90211.info()

# calculating biggest forecasted rise in yhat from present (2021-02-28) to future date (2029-12-31)

dates = ['2021-02-28','2029-12-31']

yhat_91108 = forecast_91108[['ds','yhat']]
yhat_91108 = yhat_91108[yhat_91108['ds'].isin(dates)]
yhat_91108 = yhat_91108.set_index('ds')
yhat_91108 = yhat_91108.T
yhat_91108['growth'] = yhat_91108['2029-12-31'] - yhat_91108['2021-02-28']
# yhat_91108['pct_growth'] = yhat_91108['growth']/yhat_91108['2021-02-28']

yhat_90211 = forecast_90211[['ds','yhat']]
yhat_90211 = yhat_90211[yhat_90211['ds'].isin(dates)]
yhat_90211 = yhat_90211.set_index('ds')
yhat_90211 = yhat_90211.T
yhat_90211['growth'] = yhat_90211['2029-12-31'] - yhat_90211['2021-02-28']
# yhat_90211['pct_growth'] = yhat_90211['growth']/yhat_90211['2021-02-28']

yhat_90403 = forecast_90403[['ds','yhat']]
yhat_90403 = yhat_90403[yhat_90403['ds'].isin(dates)]
yhat_90403 = yhat_90403.set_index('ds')
yhat_90403 = yhat_90403.T
yhat_90403['growth'] = yhat_90403['2029-12-31'] - yhat_90403['2021-02-28']
# yhat_90403['pct_growth'] = yhat_90403['growth']/yhat_90403['2021-02-28']

yhat_94610 = forecast_94610[['ds','yhat']]
yhat_94610 = yhat_94610[yhat_94610['ds'].isin(dates)]
yhat_94610 = yhat_94610.set_index('ds')
yhat_94610 = yhat_94610.T
yhat_94610['growth'] = yhat_94610['2029-12-31'] - yhat_94610['2021-02-28']
# yhat_91108['pct_growth'] = yhat_91108['growth']/yhat_91108['2021-02-28']

yhat_90266 = forecast_90266[['ds','yhat']]
yhat_90266 = yhat_90266[yhat_90266['ds'].isin(dates)]
yhat_90266 = yhat_90266.set_index('ds')
yhat_90266 = yhat_90266.T
yhat_90266['growth'] = yhat_90266['2029-12-31'] - yhat_90266['2021-02-28']
# yhat_90266['pct_growth'] = yhat_90266['growth']/yhat_90266['2021-02-28']

# unknown reason why the last line gives an error - similar code works earlier. Manual workaround required (below)

# examining growth values for each zip code

yhat_91108.head()

yhat_90211.head()

yhat_90403.head()

yhat_94610.head()

yhat_90266.head()

# manually calculating percent growth due to unknown error in calculation above

yhat_91108['pct_growth'] = 2.342375e+06 / 2.573679e+06
yhat_90211['pct_growth'] = 2.389851e+06 / 2.548168e+06
yhat_90403['pct_growth'] = 2.544987e+06 / 2.915727e+06
yhat_94610['pct_growth'] = 1.444374e+06 / 1.842495e+06
yhat_90266['pct_growth'] = 2.857695e+06 / 2.343886e+06

# creating df with top 5 zip codes and their percent growth

data = [['91108',.910127], ['90211',.93787], ['90403',.872848], ['94610',.783923], ['90266',1.219212]]

invest_decision = pd.DataFrame(data, columns=['Zip','Pct growth'])

invest_decision = invest_decision.sort_values(by = ['Pct growth'], ascending=False)

invest_decision.head()

"""Decision:

Invest in properties located in Zip Codes 90266, 90211, 91108
"""

# plot showing forecast over time for zip code 90266, with high and low values

model_90266.plot_components(forecast_90266)

# how good are the models? comparing 2018-2020 forecasted results to test set for zip 90266

acc = pd.merge(test_90266, forecast_90266[['ds','yhat']], on = "ds", how="left")

# calculating the difference between predicted and actual values

acc['diff'] = acc['y'] - acc['yhat']
acc['diff'] = acc['diff'].abs()
acc['error'] = acc['diff']/acc['y']

acc.head()

# Mean error for model

acc['error'].mean() * 100

# 2.7% mean error

# plotting actual home values vs predicted home values
# blue = actual, red = predicted

plt.plot('ds','y',data=acc,color='blue')
plt.plot('ds','yhat',data=acc, color = 'red')
plt.xlabel('Year')
plt.ylabel('Mean Home Value (in millions of $)')
plt.title('Comparing Model Forecast to Test Data Set')
plt.xticks(rotation = 45)
plt.show()
